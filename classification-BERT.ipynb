{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0b83b30-6d1e-44d3-83c4-04056d6389ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "from torch.optim import AdamW\n",
    "from ax.service.managed_loop import optimize\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from torch.nn import Dropout\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c12b733-09c2-4a8c-bb6d-98dacc842f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_traintest_dataset(group):\n",
    "    # Load the arguments and labels into separate DataFrames\n",
    "    df_arguments = pd.read_csv('arguments-'+group+'.tsv', delimiter='\\t')\n",
    "    df_labels = pd.read_csv('labels-'+group+'.tsv', delimiter='\\t')\n",
    "\n",
    "    # Merge the two DataFrames on the 'Argument ID' column\n",
    "    df = pd.merge(df_arguments, df_labels, on='Argument ID')\n",
    "\n",
    "    # Extract the argument text from each DataFrame\n",
    "    id = df_arguments['Argument ID'].tolist()\n",
    "    arguments = df_arguments['Premise'].tolist()\n",
    "    stances = df_arguments['Stance'].tolist()\n",
    "    conclusions = df_arguments['Conclusion'].tolist()\n",
    "\n",
    "    return arguments, stances, conclusions, id, df, df_labels\n",
    "\n",
    "labels_training = load_traintest_dataset(\"training\")[5]\n",
    "labels_test = load_traintest_dataset(\"test\")[5]\n",
    "\n",
    "# Load training dataset\n",
    "arguments_training, stances_training, conclusions_training, id_training, df_training, df_labels = load_traintest_dataset(\"training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbb53019-ea8d-4d40-8c01-180f8444b52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = text.split()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    text = ' '.join(words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4eaff062-d702-40bf-9c36-05a328e1c154",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(test, training, columns, train_labels, test_labels):\n",
    "    # Define the hyperparameters\n",
    "    hyperparameters = {\n",
    "        'learning_rate': [1e-5, 3e-5, 1e-4],\n",
    "        'batch_size': [16, 32],\n",
    "        'dropout_rate': [0.1, 0.2, 0.3]\n",
    "    }\n",
    "\n",
    "    # Initialize the random search\n",
    "    random_search = ParameterSampler(hyperparameters, n_iter=10, random_state=0)\n",
    "\n",
    "    best_f1 = 0\n",
    "    best_model = None\n",
    "\n",
    "    # Loop over each hyperparameter configuration\n",
    "    for params in random_search:\n",
    "        learning_rate = params['learning_rate']\n",
    "        batch_size = params['batch_size']\n",
    "        dropout_rate = params['dropout_rate']\n",
    "\n",
    "        test_df = pd.read_csv(test, sep=\"\\t\")\n",
    "        training_df = pd.read_csv(training, sep=\"\\t\")\n",
    "\n",
    "        test_df['combined'] = test_df[columns].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
    "        training_df['combined'] = training_df[columns].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
    "        expanded_text_test = test_df['combined'].tolist()\n",
    "        expanded_text_training = training_df['combined'].tolist()\n",
    "\n",
    "        # Preprocess the text\n",
    "        expanded_text_test_preprocessed = [preprocess_text(text) for text in expanded_text_test]\n",
    "        expanded_text_training_preprocessed = [preprocess_text(text) for text in expanded_text_training]\n",
    "\n",
    "        # Tokenize the texts\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        inputs_train = tokenizer(training_df['combined'].tolist(), return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        inputs_test = tokenizer(test_df['combined'].tolist(), return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "        # Create TensorDatasets\n",
    "        dataset_train = TensorDataset(inputs_train['input_ids'], inputs_train['attention_mask'], torch.tensor(train_labels))\n",
    "        dataset_test = TensorDataset(inputs_test['input_ids'], inputs_test['attention_mask'], torch.tensor(test_labels))\n",
    "\n",
    "        # Create DataLoaders\n",
    "        dataloader_train = DataLoader(dataset_train, batch_size=batch_size)\n",
    "        dataloader_test = DataLoader(dataset_test, batch_size=batch_size)\n",
    "\n",
    "        # Initialize the model and optimizer\n",
    "        model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=20)  # For the 20 value labels\n",
    "        model.to('cpu')\n",
    "        optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "        # Add dropout \n",
    "        dropout = Dropout(dropout_rate)\n",
    "\n",
    "        # Use BCEWithLogitsLoss for multi-label classification\n",
    "        criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "        # Train model\n",
    "        for epoch in range(5):\n",
    "            for batch in dataloader_train:\n",
    "                input_ids, attention_mask, labels = batch\n",
    "                input_ids = input_ids.to('cpu')\n",
    "                attention_mask = attention_mask.to('cpu')\n",
    "                labels = labels.to('cpu').float()\n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "                outputs = dropout(outputs.logits)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "        # Evaluate model\n",
    "        model.eval()\n",
    "        predictions = []\n",
    "        true_labels = []\n",
    "        for batch in dataloader_test:\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            input_ids = input_ids.to('cpu')\n",
    "            attention_mask = attention_mask.to('cpu')\n",
    "            labels = labels.to('cpu')\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            predictions.append(logits)\n",
    "            true_labels.append(labels)\n",
    "\n",
    "        predictions = torch.cat(predictions, dim=0)\n",
    "        true_labels = torch.cat(true_labels, dim=0)\n",
    "\n",
    "        # Calculate the metrics\n",
    "        f1 = f1_score(true_labels.cpu(), predictions.cpu() > 0, average='samples')\n",
    "        precision = precision_score(true_labels.cpu(), predictions.cpu() > 0, average='samples', zero_division=0)\n",
    "        recall = recall_score(true_labels.cpu(), predictions.cpu() > 0, average='samples')\n",
    "\n",
    "        # If the current model is better than the previous best model, update the best model\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_model = model\n",
    "\n",
    "    return best_model, best_f1, precision, recall, test_df['combined'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "265c2dcc-40d3-4fcd-8505-c1d58cca4e23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m model_original, f1_original, precision_original, recall_original, expanded_text_test_preprocessed_original \u001b[38;5;241m=\u001b[39m train_and_evaluate(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marguments-test.tsv\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marguments-training.tsv\u001b[39m\u001b[38;5;124m\"\u001b[39m, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConclusion\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStance\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPremise\u001b[39m\u001b[38;5;124m\"\u001b[39m], labels_training_array, labels_test_array)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Train and evaluate the model on the expanded dataset\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m model_expanded, f1_expanded, precision_expanded, recall_expanded, expanded_text_test_preprocessed_expanded \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHVD-expanded_dataset_test.tsv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHVD-expanded_dataset_training.tsv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEXPANDED TEXT\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_training_array\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_test_array\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Train and evaluate the model on the expanded dataset with adjectives\u001b[39;00m\n\u001b[0;32m     16\u001b[0m model_expanded_adj, f1_expanded_adj, precision_expanded_adj, recall_expanded_adj, expanded_text_test_preprocessed_expanded_adj \u001b[38;5;241m=\u001b[39m train_and_evaluate(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHVD-expanded_dataset_test.tsv\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHVD-expanded_dataset_training.tsv\u001b[39m\u001b[38;5;124m\"\u001b[39m, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEXPANDED TEXT ADJECTIVES\u001b[39m\u001b[38;5;124m\"\u001b[39m], labels_training_array, labels_test_array)\n",
      "Cell \u001b[1;32mIn[4], line 66\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[1;34m(test, training, columns, train_labels, test_labels)\u001b[0m\n\u001b[0;32m     64\u001b[0m outputs \u001b[38;5;241m=\u001b[39m dropout(outputs\u001b[38;5;241m.\u001b[39mlogits)\n\u001b[0;32m     65\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m---> 66\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     68\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load labels\n",
    "labels_training = pd.read_csv(\"labels-training.tsv\", sep=\"\\t\")\n",
    "labels_test = pd.read_csv(\"labels-test.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Convert labels to the correct format\n",
    "labels_training_array = labels_training.drop('Argument ID', axis=1).values\n",
    "labels_test_array = labels_test.drop('Argument ID', axis=1).values\n",
    "\n",
    "# Train and evaluate the model on the original dataset\n",
    "model_original, f1_original, precision_original, recall_original, expanded_text_test_preprocessed_original = train_and_evaluate(\"arguments-test.tsv\", \"arguments-training.tsv\", [\"Conclusion\", \"Stance\", \"Premise\"], labels_training_array, labels_test_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80937631-9ac3-4a1e-92f1-d485c6834f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset:\n",
      "F1 Score: 0.5462076276797089\n",
      "Precision: 0.6350888324873096\n",
      "Recall: 0.36385439328015473\n"
     ]
    }
   ],
   "source": [
    "# Print the evaluation metrics for each model\n",
    "print(\"Original dataset:\")\n",
    "print(f\"F1 Score: {f1_original}\")\n",
    "print(f\"Precision: {precision_original}\")\n",
    "print(f\"Recall: {recall_original}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d86aa65-492c-4842-b333-b64f13c942f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train and evaluate the model on the expanded dataset\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model_expanded, f1_expanded, precision_expanded, recall_expanded, expanded_text_test_preprocessed_expanded \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHVD-expanded_dataset_test.tsv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHVD-expanded_dataset_training.tsv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEXPANDED TEXT\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_training_array\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_test_array\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpanded dataset:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF1 Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf1_expanded\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[4], line 66\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[1;34m(test, training, columns, train_labels, test_labels)\u001b[0m\n\u001b[0;32m     64\u001b[0m outputs \u001b[38;5;241m=\u001b[39m dropout(outputs\u001b[38;5;241m.\u001b[39mlogits)\n\u001b[0;32m     65\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m---> 66\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     68\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train and evaluate the model on the expanded dataset\n",
    "model_expanded, f1_expanded, precision_expanded, recall_expanded, expanded_text_test_preprocessed_expanded = train_and_evaluate(\"HVD-expanded_dataset_test.tsv\", \"HVD-expanded_dataset_training.tsv\", [\"EXPANDED TEXT\"], labels_training_array, labels_test_array)\n",
    "\n",
    "print(\"Expanded dataset:\")\n",
    "print(f\"F1 Score: {f1_expanded}\")\n",
    "print(f\"Precision: {precision_expanded}\")\n",
    "print(f\"Recall: {recall_expanded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642ced94-45ce-4f8a-acb7-9ebb90363f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate the model on the expanded dataset with adjectives\n",
    "model_expanded_adj, f1_expanded_adj, precision_expanded_adj, recall_expanded_adj, expanded_text_test_preprocessed_expanded_adj = train_and_evaluate(\"HVD-expanded_dataset_test.tsv\", \"HVD-expanded_dataset_training.tsv\", [\"EXPANDED TEXT ADJECTIVES\"], labels_training_array, labels_test_array)\n",
    "\n",
    "print(\"Expanded dataset with adjectives:\")\n",
    "print(f\"F1 Score: {f1_expanded_adj}\")\n",
    "print(f\"Precision: {precision_expanded_adj}\")\n",
    "print(f\"Recall: {recall_expanded_adj}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336092b0-549c-43a3-ab6b-fcde31811e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the metrics\n",
    "models = ['Original dataset', 'Free approach', 'Guided approach']\n",
    "precision_vals = [precision_original, precision_expanded, precision_expanded_adj]\n",
    "recall_vals = [recall_original, recall_expanded, recall_expanded_adj]\n",
    "f1_vals = [f1_original, f1_expanded, f1_expanded_adj]\n",
    "\n",
    "x = np.arange(len(models)) \n",
    "width = 0.3\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "rects1 = ax.bar(x - width, precision_vals, width, label='Precision')\n",
    "rects2 = ax.bar(x, recall_vals, width, label='Recall')\n",
    "rects3 = ax.bar(x + width, f1_vals, width, label='F1 Score')\n",
    "\n",
    "ax.set_xlabel('Models')\n",
    "ax.set_ylabel('Scores')\n",
    "ax.set_title('Scores by model')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(models)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf53999f-0025-4feb-9a5d-8fb057c29a41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a3c667-e0ba-4c48-b1c3-7fa6a51008dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
