{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0ecedca1-975c-4d02-a5b7-aa2b7bd396e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import transformers\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "import string\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "\n",
    "from valuemap.models import Model, MultiModel\n",
    "from valuemap.values import ValueMap, ValueSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c3a9d012-07a3-47fb-aad4-e062586cbc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    # Load the dataset into separate DataFrames for each split\n",
    "    df_training = pd.read_csv('arguments-training.tsv', delimiter='\\t')\n",
    "    df_validation = pd.read_csv('arguments-validation.tsv', delimiter='\\t')\n",
    "    df_test = pd.read_csv('arguments-test.tsv', delimiter='\\t')\n",
    "\n",
    "    # Concatenate all the dataframes\n",
    "    df = pd.concat([df_training, df_validation, df_test])\n",
    "\n",
    "    # Extract the argument text from each DataFrame\n",
    "    arguments = df['Premise'].tolist()\n",
    "    stances = df['Stance'].tolist()\n",
    "    conclusions = df['Conclusion'].tolist()\n",
    "\n",
    "    return arguments, stances, conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c4805330-92ed-404d-880e-c944152b0436",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sentences(sentences):\n",
    "    processed = []\n",
    "    for sentence in sentences:\n",
    "        # Remove punctuation\n",
    "        sentence = sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "        # Convert to lowercase\n",
    "        sentence = sentence.lower()\n",
    "        processed.append(sentence)\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8cff5be1-396e-4f70-8b4a-fa89e5125a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# model inizialization\n",
    "model_bert = pipeline('fill-mask', model='bert-base-uncased') # Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7cc040b4-cf47-4a7d-9b4d-c1bcb4a87a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# model inizialization for adjectiveList-based approach\n",
    "def load_model() :\n",
    "    # Load the model and tokenizer\n",
    "    model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    return model, tokenizer\n",
    "\n",
    "model, tokenizer = load_model()[0], load_model()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0d1d7350-29d8-4524-b404-c678492e2e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "arguments, stances, conclusions = load_dataset()\n",
    "\n",
    "# Preprocess premise and conclusion\n",
    "arguments = process_sentences(arguments)\n",
    "conclusions = process_sentences(conclusions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4f446467-1209-42f5-9398-9c82e6a0caa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map each argument to the corresponding stance and conclusion\n",
    "if len(arguments) != len(stances) or len(arguments) != len(conclusions):\n",
    "    print(\"Error: incompatible data\")\n",
    "else:\n",
    "    input = {}\n",
    "    for i in range(len(arguments)):\n",
    "        input[arguments[i]] = (stances[i], conclusions[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "31920465-84e1-4429-8b46-993ec3817cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Random argument selection\n",
    "input_argument = random.choice(list(input.keys()))\n",
    "\n",
    "argument = input_argument\n",
    "stance = input[input_argument][0]\n",
    "conclusion = input[input_argument][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "fa7c378b-673d-47a7-93f1-029803388f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_word(model):\n",
    "    # Initialize an empty list to store the results\n",
    "    results = []\n",
    "\n",
    "    # Create prompt \n",
    "    prompt = f\"i am {stance} the fact that {conclusion} because i think that {argument}. i am a {model.tokenizer.mask_token}.\"\n",
    "    \n",
    "    # Generate the filling\n",
    "    output = model(prompt)\n",
    "   \n",
    "    # Sort output by 'score'\n",
    "    output.sort(key=lambda x: x['score'], reverse=True)\n",
    "    \n",
    "    # Create a dictionary of other predicted words with their scores\n",
    "    predictions = {result['token_str']: result['score'] for result in output}\n",
    "\n",
    "    # Extract 'sequence' with higher 'score'\n",
    "    description = output[0]['sequence']\n",
    "    \n",
    "    return predictions, description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "25d17414-040f-422f-8fea-f143a3a11e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'feminist': 0.1193666011095047, 'conservative': 0.07823804765939713, 'libertarian': 0.07749752700328827, 'vegetarian': 0.07096802443265915, 'christian': 0.06777476519346237}\n",
      "\n",
      "i am in favor of the fact that we should subsidize wikipedia because i think that we should subsidize wikipedia because wikipedia helps people to learn about any and everything they want to learn about. i am a feminist.\n"
     ]
    }
   ],
   "source": [
    "print(f\"{generate_word(model_bert)[0]}\\n\\n{generate_word(model_bert)[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "69e6092e-56c8-431e-8ae6-e8b4b288d33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "adjective_list = [\n",
    "    \"conservative\",\n",
    "    \"liberal\",\n",
    "    \"republican\",\n",
    "    \"libertarian\",\n",
    "    \"democrat\",\n",
    "    \"progressive\",\n",
    "    \"socialist\",\n",
    "    \"communist\",\n",
    "    \"anarchist\",\n",
    "    \"centrist\",\n",
    "    \"capitalist\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "611b9a6d-59e0-447b-8824-330719094ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_word_adj(model, tokenizer, adjective_list):\n",
    "    # Create prompt \n",
    "    prompt = f\"i am {stance} the fact that {conclusion} because i think that {argument}. i am a {tokenizer.mask_token}.\"\n",
    "    # Initialize a dictionary to store the probabilities\n",
    "    probabilities = {}\n",
    "    # For each word in the list, generate a score\n",
    "    for word in adjective_list:\n",
    "        # Replace the mask token with the word\n",
    "        new_prompt = prompt.replace(tokenizer.mask_token, word)\n",
    "        # Encode the new prompt\n",
    "        inputs = tokenizer.encode_plus(new_prompt, return_tensors='pt')\n",
    "        # Generate the filling\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        # Calculate the softmax probabilities from logits\n",
    "        softmax_probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        # Get the probability of the word\n",
    "        word_id = tokenizer.encode(word, add_special_tokens=False)[0]\n",
    "        word_prob = softmax_probs[0, -1, word_id].item()\n",
    "\n",
    "        # Store the probability of the word\n",
    "        probabilities[word] = word_prob\n",
    "\n",
    "    # Choose the word with the highest probability\n",
    "    top_word = max(probabilities, key=probabilities.get)\n",
    "\n",
    "    # Replace the mask token in the original prompt with the top word\n",
    "    description = prompt.replace(tokenizer.mask_token, top_word)\n",
    "\n",
    "    return probabilities, description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "42ca0aae-569b-4e2e-bdef-d90589d86c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'conservative': 8.789901584371762e-10, 'liberal': 6.636675653481916e-09, 'republican': 2.369497931198339e-09, 'libertarian': 8.685015207010593e-11, 'democrat': 1.2124306003613583e-09, 'progressive': 2.527806436392055e-11, 'socialist': 1.3472681303916545e-10, 'communist': 6.878602132331935e-10, 'anarchist': 1.27436776409251e-11, 'centrist': 3.00495874927878e-13, 'capitalist': 1.6383137901865297e-10}\n",
      "\n",
      "i am in favor of the fact that we should subsidize wikipedia because i think that we should subsidize wikipedia because wikipedia helps people to learn about any and everything they want to learn about. i am a liberal.\n"
     ]
    }
   ],
   "source": [
    "print(f\"{generate_word_adj(model, tokenizer, adjective_list)[0]}\\n\\n{generate_word_adj(model, tokenizer, adjective_list)[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09070606-a23e-4dcf-9f08-f78f0b29de44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a53a8b-c65b-4130-917b-c55a976aa07a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc6190d-866c-4bfb-8a67-3e3f08685223",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
